{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b638889-ba64-4263-86d8-8eef0af656e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "accelerate                0.29.3\n",
      "aiohttp                   3.9.5\n",
      "aiosignal                 1.3.1\n",
      "anyio                     4.3.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "async-timeout             4.0.3\n",
      "attrs                     23.2.0\n",
      "Babel                     2.14.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "certifi                   2024.2.2\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "click                     8.1.7\n",
      "comm                      0.2.2\n",
      "datasets                  2.19.0\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "dill                      0.3.8\n",
      "evaluate                  0.4.1\n",
      "exceptiongroup            1.2.1\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.19.1\n",
      "filelock                  3.13.4\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.4.1\n",
      "fsspec                    2024.3.1\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.5\n",
      "httpx                     0.27.0\n",
      "huggingface-hub           0.22.2\n",
      "idna                      3.7\n",
      "ipykernel                 6.29.4\n",
      "ipython                   8.23.0\n",
      "ipywidgets                8.1.2\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.3\n",
      "joblib                    1.4.0\n",
      "json5                     0.9.25\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.21.1\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter                   1.0.0\n",
      "jupyter_client            8.6.1\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.1.6\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.26.0\n",
      "jupyterlab_widgets        3.0.10\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.0.2\n",
      "mpmath                    1.3.0\n",
      "multidict                 6.0.5\n",
      "multiprocess              0.70.16\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.3\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.3\n",
      "nltk                      3.8.1\n",
      "notebook                  7.1.3\n",
      "notebook_shim             0.2.4\n",
      "numpy                     1.26.4\n",
      "nvidia-cublas-cu12        12.1.3.1\n",
      "nvidia-cuda-cupti-cu12    12.1.105\n",
      "nvidia-cuda-nvrtc-cu12    12.1.105\n",
      "nvidia-cuda-runtime-cu12  12.1.105\n",
      "nvidia-cudnn-cu12         8.9.2.26\n",
      "nvidia-cufft-cu12         11.0.2.54\n",
      "nvidia-curand-cu12        10.3.2.106\n",
      "nvidia-cusolver-cu12      11.4.5.107\n",
      "nvidia-cusparse-cu12      12.1.0.106\n",
      "nvidia-nccl-cu12          2.19.3\n",
      "nvidia-nvjitlink-cu12     12.4.127\n",
      "nvidia-nvtx-cu12          12.1.105\n",
      "overrides                 7.7.0\n",
      "packaging                 24.0\n",
      "pandas                    2.2.2\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pip                       22.0.2\n",
      "platformdirs              4.2.0\n",
      "prometheus_client         0.20.0\n",
      "prompt-toolkit            3.0.43\n",
      "psutil                    5.9.8\n",
      "ptyprocess                0.7.0\n",
      "pure-eval                 0.2.2\n",
      "pyarrow                   15.0.2\n",
      "pyarrow-hotfix            0.6\n",
      "pycparser                 2.22\n",
      "Pygments                  2.17.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2024.1\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     26.0.2\n",
      "qtconsole                 5.5.1\n",
      "QtPy                      2.4.1\n",
      "referencing               0.34.0\n",
      "regex                     2024.4.16\n",
      "requests                  2.31.0\n",
      "responses                 0.18.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.18.0\n",
      "safetensors               0.4.3\n",
      "scikit-learn              1.4.2\n",
      "scipy                     1.13.0\n",
      "Send2Trash                1.8.3\n",
      "setuptools                59.6.0\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "stack-data                0.6.3\n",
      "sympy                     1.12\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.4.0\n",
      "tinycss2                  1.2.1\n",
      "tokenizers                0.19.1\n",
      "tomli                     2.0.1\n",
      "torch                     2.2.2\n",
      "tornado                   6.4\n",
      "tqdm                      4.66.2\n",
      "traitlets                 5.14.3\n",
      "transformers              4.40.0\n",
      "triton                    2.2.0\n",
      "types-python-dateutil     2.9.0.20240316\n",
      "typing_extensions         4.11.0\n",
      "tzdata                    2024.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.1\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.7.0\n",
      "wheel                     0.37.1\n",
      "widgetsnbextension        4.0.10\n",
      "xxhash                    3.4.1\n",
      "yarl                      1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b24a5804-4600-48d1-b7cc-da64e894048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/lib/python3.10/site-packages (4.40.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: evaluate in ./venv/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (1.4.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers pandas datasets nltk beautifulsoup4 torch evaluate transformers[torch] scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34234d5c-b0c7-4943-ac1f-4ffa2780eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "target_directory = '/export/scratch/khond014-sentiment'\n",
    "\n",
    "os.chdir(target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d911fa26-f490-4b0e-856c-b07f5ea19a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /export/scratch/khond014-sentiment...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /export/scratch/khond014-sentiment...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk_data_path = '/export/scratch/khond014-sentiment'\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab1fdab-1c4d-41e9-b32c-00d0f0c1e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, pipeline\n",
    "\n",
    "from datasets import load_dataset\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a139791-a91f-4da7-88ea-42ce0fb92da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c301c764da3748cf969c2691527d01dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e564144c-d314-4d00-aa1d-0ef8312545ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12914e559ce42d4b68aa6a3437471fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92704bd7f8d4743b566afebf4522922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57bca36280a410cb68d03ee9b3bbf00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46dec0d152e4495889b012993d661e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fcd2af2c9442c6b48868bac37a2e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a36cbc890e4f398c8bd805786b311b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71feb6072b8445b6bc31f8b2b53e4bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset and specify the data directory\n",
    "dataset = load_dataset('stanfordnlp/imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "417f2ee8-f04f-44c4-a867-171a86521c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(example):\n",
    "    # Extract the 'text' field from the example\n",
    "    text = example['text']\n",
    "\n",
    "    # Parse HTML content and remove tags\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text_without_html = soup.get_text(separator=' ')\n",
    "    text_without_html = re.sub(r'\\s+', ' ', text_without_html).strip()\n",
    "\n",
    "    # Update the 'text' field in the example with processed text\n",
    "    example['text'] = text_without_html\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7476ab13-52e7-4175-9cf9-c91683558843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(example):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(example['text'])\n",
    "\n",
    "    # Define stopwords, punctuation characters, and contractions\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    contractions = {\n",
    "        \"n't\": \"not\",\n",
    "        \"'s\": \"\",\n",
    "        \"'re\": \"are\",\n",
    "        \"'ve\": \"have\",\n",
    "        \"'d\": \"would\",\n",
    "        \"'ll\": \"will\",\n",
    "        \"'m\": \"am\"\n",
    "    }\n",
    "\n",
    "    # Filter out stopwords, punctuation, and handle contractions\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token_lower = token.lower()\n",
    "        if token_lower in contractions:\n",
    "            cleaned_token = contractions[token_lower]\n",
    "            if cleaned_token != \"\":\n",
    "                filtered_tokens.append(cleaned_token)\n",
    "        elif token_lower not in stop_words and token_lower not in punctuation:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    # Reconstruct the text from filtered tokens\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    # Update the 'text' field in the example with cleaned text\n",
    "    example['text'] = cleaned_text\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fea45c-0ffd-4921-8f56-30e6e3835a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review_text(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Define stopwords, punctuation characters, and contractions\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    contractions = {\n",
    "        \"n't\": \"not\",\n",
    "        \"'s\": \"\",\n",
    "        \"'re\": \"are\",\n",
    "        \"'ve\": \"have\",\n",
    "        \"'d\": \"would\",\n",
    "        \"'ll\": \"will\",\n",
    "        \"'m\": \"am\"\n",
    "    }\n",
    "\n",
    "    # Filter out stopwords, punctuation, and handle contractions\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token_lower = token.lower()\n",
    "        if token_lower in contractions:\n",
    "            cleaned_token = contractions[token_lower]\n",
    "            if cleaned_token != \"\":\n",
    "                filtered_tokens.append(cleaned_token)\n",
    "        elif token_lower not in stop_words and token_lower not in punctuation:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    # Reconstruct the text from filtered tokens\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "084f1129-f040-42d6-ae84-b60f296f8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a6f8c26-a1de-4eab-a950-47fd0dfe0c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6644f1b12a9c4aac8dc5118792c9f318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b46cfd9baca4715949ba6422d7c9ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe03d4a4aa94b59a052a766978c28ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db42b6feb3843d286952901afa29e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed633bb93c84bc8ae694907938c71e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL, num_labels=2, id2label=id2label, label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa3b1453-ba0b-45f2-b8c7-b149239c8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tokenizer(examples):\n",
    "     return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3480222-b1c5-4d1f-becb-d3602998e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_to_process = ['train', 'test', 'unsupervised']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be8a882d-2ad4-4be8-8542-461ec5516d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9df57cfd713468bbddc652cae1ec719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1330160/781736049.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n",
      "Parameter 'function'=<function clean_review at 0x7fe191734b80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f338a784e9b54fe3832f24ab0b62b6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b411b9573afe46a18cac974b9e5d27ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0fea259d4343ce87ed4b3f755d7974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4e4941f9924ce29eef1dbb6480611c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e451208a98be44a283e336778186963a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Batch size for processing\n",
    "batch_size=1000\n",
    "\n",
    "new_dataset = dataset\n",
    "\n",
    "for split in splits_to_process:\n",
    "    # Apply the remove_html_tags function to the 'text' column in the specified split\n",
    "    new_dataset[split] = new_dataset[split].map(remove_html_tags)\n",
    "    new_dataset[split] = new_dataset[split].map(clean_review)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acedc79d-c834-4e76-b215-fe7514596b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b526151fe64ca69dadbea55edb7584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec028111eb8e4173ae7f580b9c1abd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79802b378ef429c85cd45b3750bf8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = new_dataset.map(model_tokenizer, batched=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47011f74-d14b-400f-971d-f483ba286fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19322745-69f6-4f95-8d13-e4fc8808de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18656944-c2fa-4549-8f53-4da9e2f062c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LOOOGITS!!\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ebade35-9473-4a00-a01c-e017306108a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 2:34:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>0.362881</td>\n",
       "      <td>0.918280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>0.286519</td>\n",
       "      <td>0.929160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>0.370394</td>\n",
       "      <td>0.930280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.2298971268717448, metrics={'train_runtime': 9278.0379, 'train_samples_per_second': 8.084, 'train_steps_per_second': 1.01, 'total_flos': 1.9733329152e+16, 'train_loss': 0.2298971268717448, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_ling_twitter_full\",\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af69f2f-5eb5-4d7d-9d02-b5bd10e54b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
